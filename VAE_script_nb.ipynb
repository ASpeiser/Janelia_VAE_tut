{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal script to train a variational auto-encoder on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T15:45:54.769568",
     "start_time": "2016-09-28T15:45:46.915303"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "#from theano import *\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from theano import config\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import functions\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "config.warn_float64 = 'warn'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters and load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T15:45:54.941118",
     "start_time": "2016-09-28T15:45:54.771164"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n"
     ]
    }
   ],
   "source": [
    "HU_enc = 500  # Size of the hidden Layers for the decoding/encoding network\n",
    "HU_dec = 500\n",
    "dimZ   = 2    # Dimensionality of your latent space (chose 2 if you want to visualize the result)\n",
    "b_size = 100  # Batch Size\n",
    "epochs = 1300 # Training epochs (number of iterations thorugh the whole dataset)\n",
    "\n",
    "datasets = functions.load_data_np(os.path.abspath('mnist.pkl'))\n",
    "\n",
    "train_set_x, _ = datasets[0] # We only take the images and not the labels as we don't do classification\n",
    "valid_set_x, _ = datasets[1]\n",
    "\n",
    "N = train_set_x.shape[0]     # Number of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T15:45:55.009046",
     "start_time": "2016-09-28T15:45:54.943220"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "    \n",
    "def init_weights(shape, scale = 0.01, name = None):\n",
    "    \"\"\" Initialize random gaussian weights \"\"\"\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * scale), name = name)\n",
    "    \n",
    "def init_biases(shape, scale = 0.01, name = None):\n",
    "    \"\"\" Initialize random gaussian biases. Broadcasting allows to concatentate the bias to match the batchsize \"\"\"\n",
    "    return theano.shared(floatX(np.random.randn(shape,1)* scale), name = name, broadcastable=(False, True))\n",
    "\n",
    "X = T.fmatrix() #T Theanotensors for the data and latent variables\n",
    "Z = T.fmatrix()\n",
    "\n",
    "W1 = init_weights((HU_enc, 28*28), name = 'W1') # 28*28 is the image size\n",
    "b1 = init_biases((HU_enc), name = 'b1')\n",
    "\n",
    "W2 = init_weights((dimZ, HU_enc), name = 'W2')\n",
    "b2 = init_biases((dimZ), name = 'b2')\n",
    "\n",
    "W3 = init_weights((dimZ, HU_enc), name = 'W3')\n",
    "b3 = init_biases((dimZ), name = 'b3')\n",
    "\n",
    "W4 = init_weights((HU_dec, dimZ), name = 'W4')\n",
    "b4 = init_biases((HU_dec), name = 'b4')\n",
    "\n",
    "W5 = init_weights((28*28, HU_dec), name = 'W5')\n",
    "b5 = init_biases((28*28), name = 'b5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T15:45:55.037053",
     "start_time": "2016-09-28T15:45:55.010357"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def model(x, W1, W2, W3, W4, W5, b1, b2, b3, b4, b5, batch_size):\n",
    "\n",
    "    eps = theano.shared(floatX(np.random.randn(dimZ, batch_size))) # Epsilon is drawn from a gaussian to sample from z\n",
    "                                                                   # for the reparametrization trick\n",
    "        \n",
    "    h_enc =         T.tanh(T.dot(W1,x) + b1)                       # encoding part\n",
    "    mu_enc =        T.dot(W2,h_enc) + b2\n",
    "    log_sig_enc =   0.5*(T.dot(W3,h_enc) + b3)\n",
    "    \n",
    "    z =             mu_enc + T.exp(log_sig_enc)*eps                # latent var.\n",
    "    \n",
    "    prior = 0.5 *   T.sum(1 + 2*log_sig_enc - mu_enc**2 - T.exp(3*log_sig_enc)) # regularizing term of the lower bound \n",
    "    \n",
    "    h_dec =         T.tanh(T.dot(W4,z) + b4)                      # decoding part for binary variables\n",
    "    y =             T.nnet.sigmoid(T.dot(W5,h_dec) + b5)          # y is the probability\n",
    "    logpxz =        T.sum(x*T.log(y) + (1 - x)*T.log(1 - y))      # reconstruction term   \n",
    "    \n",
    "    bound = (prior + logpxz)/batch_size                           # lower bound\n",
    "    \n",
    "    return bound\n",
    "\n",
    "cost = model(X, W1, W2, W3, W4, W5, b1, b2, b3, b4, b5, b_size)   # The cost that gets minimized is just the lower bound\n",
    "params = [W1, W2, W3, W4, W5, b1, b2, b3, b4, b5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-09-28T13:45:46.938Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def adagrad_w_prior(loss_or_grads, params, factor, learning_rate=1.0, epsilon=1e-6):\n",
    "    \"\"\"See lasagne.updates.adagrad for details. I copied the function and added a prior to regularize the weights\"\"\"\n",
    "    grads = theano.grad(loss_or_grads, params)\n",
    "    updates = OrderedDict()\n",
    "\n",
    "    for param, grad in zip(params, grads):\n",
    "        value = param.get_value(borrow=True)\n",
    "        accu = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                             broadcastable=param.broadcastable)\n",
    "        accu_new = accu + grad ** 2\n",
    "        updates[accu] = accu_new\n",
    "        \n",
    "        if param.name.count('W') == 1: # Weights are regularized, but not biases.\n",
    "            updates[param] = param - (learning_rate * (grad + 2*factor*param) / T.sqrt(accu_new + epsilon))\n",
    "        else:\n",
    "            updates[param] = param - (learning_rate * (grad / T.sqrt(accu_new + epsilon)))\n",
    "\n",
    "    return updates\n",
    "\n",
    "updates = adagrad_w_prior(-cost , params, b_size/N ,learning_rate = 0.01)\n",
    "\n",
    "train = theano.function(inputs=[X], updates=updates, allow_input_downcast=True)  # Trains and updates the weights\n",
    "valid = theano.function(inputs=[X], outputs= cost, allow_input_downcast=True)    # Returns the lower bound (for readout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-09-28T13:45:46.950Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -184.879168549 2.872445583343506\n"
     ]
    }
   ],
   "source": [
    "bound_train = np.empty(epochs)\n",
    "bound_valid = np.empty(epochs)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    t0 = time.time()        \n",
    "    \n",
    "    bound_t = 0\n",
    "    bound_v = 0\n",
    "    \n",
    "    #For every batch, update the parameters and calculate the bound for the training and validation set. \n",
    "    for start, end in zip(range(0, len(train_set_x), b_size), range(b_size, len(train_set_x), b_size)):\n",
    "        train(train_set_x[start:end].T)\n",
    "        \n",
    "    for start, end in zip(range(0, len(train_set_x), b_size), range(b_size, len(train_set_x), b_size)):\n",
    "        bound_t += valid(train_set_x[start:end].T)   \n",
    "        \n",
    "    for start, end in zip(range(0, len(valid_set_x), b_size), range(b_size, len(valid_set_x), b_size)):\n",
    "        bound_v += valid(valid_set_x[start:end].T)          \n",
    "\n",
    "    bound_train[i] = bound_t/(len(train_set_x)/b_size)    \n",
    "    bound_valid[i] = bound_v/(len(valid_set_x)/b_size) \n",
    "    timing         = time.time()-t0          # Time per epoch\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(i, bound_valid[i], timing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-09-28T13:45:46.955Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the lower bound\n",
    "plt.plot(N*np.arange(len(bound_train)), bound_train, linewidth=3, label=\"train\")\n",
    "plt.plot(N*np.arange(len(bound_valid)), bound_valid, linewidth=3, label=\"valid\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"samples\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim(1e5, N*len(bound_train))\n",
    "plt.ylim(-200,-130)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and load your trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-09-28T13:45:46.960Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#f = file('mnist_vae.pkl', 'wb')\n",
    "#pickle.dump(params, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#f.close()\n",
    "\n",
    "#f = file('mnist_vae.pkl', 'rb')\n",
    "#params_loaded = pickle.load(f)\n",
    "#f.close()\n",
    "#\n",
    "#W1, W2, W3, W4, W5, b1, b2, b3, b4, b5 = params_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from your latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-09-28T13:45:46.964Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def latent_rec(z, W4, W5, b4, b5):\n",
    "\n",
    "    h_dec =         T.tanh(T.dot(W4,z) + b4)\n",
    "    y =             T.nnet.sigmoid(T.dot(W5,h_dec) + b5)\n",
    "    \n",
    "    return y\n",
    "\n",
    "reconstruction = latent_rec(Z, W4, W5, b4, b5)\n",
    "latent_space = theano.function(inputs=[Z], outputs = reconstruction, allow_input_downcast=True)\n",
    "\n",
    "plt.figure(frameon=False, figsize = (12,12))\n",
    "\n",
    "gs1 = gridspec.GridSpec(19, 19)\n",
    "gs1.update(wspace=0.0, hspace=0.0) # set the spacing between axes. \n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in np.arange(0.05,1.,0.05):\n",
    "    for j in np.arange(0.05,1.,0.05):\n",
    "        plt.subplot(gs1[count])\n",
    "        z = np.array([scipy.stats.norm.ppf(i),scipy.stats.norm.ppf(j)])\n",
    "        z=z.reshape(z.shape[0],1) \n",
    "        shared_z = theano.shared(np.asarray(z,dtype=theano.config.floatX),borrow=True)\n",
    "        rec = np.reshape(latent_space(z),[28,28])\n",
    "        plt.imshow(rec)\n",
    "        plt.set_cmap('Greys')\n",
    "        plt.axis('off')\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
